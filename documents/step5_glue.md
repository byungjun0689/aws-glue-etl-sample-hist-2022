## 5. Glue

- AWS ê°œí¸ì´ ë˜ë©´ì„œ Glue ì„œë¹„ìŠ¤ ë‚´ Legacy Pagesì— ëª¨ë“  ë©”ë‰´ê°€ ë“¤ì–´ê°€ê²Œ ë˜ë©´ì„œ í•´ë‹¹ Top ë©”ë‰´ì—ì„œ ì•„ë˜ ê¸°ëŠ¥ë“¤ì„ ìƒì„±.


ğŸ’¡ `[ì„¤ëª…]`
[AWS Glueë€?](https://www.notion.so/AWS-Glue-21a4e620cac84c54a1960d5f7d801697?pvs=21)


### 5.0 Glue IAM ìƒì„±

- `IAM(ì„œë¹„ìŠ¤)` ì—­í•  ìƒì„± â†’ ì„œë¹„ìŠ¤ `Glue` ì„ íƒ
    - Secrets Manager ì ‘ê·¼ `SecretsManagerReadWrite` ê¶Œí•œ ì¶”ê°€ í•„ìš”
    - S3 ì ‘ê·¼ `AmazonS3FullAccess`
    - Glue ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆëŠ” `AWSGlueServiceRole` ì„ íƒ
    - `ì°¸ì¡°`
        - ì •ì±… ê²€ìƒ‰ í›„ ì„ íƒ â†’ í•„í„°ë§ ì§€ìš°ê¸° â†’ ë‹¤ì‹œ ê²€ìƒ‰ í˜•íƒœë¡œ ìœ„ 3ê°œ í•­ëª© ì„ íƒ.
    - ì´ë¦„
        - `{ë©”ì¼id}-handson-glue-role`

![Untitled](../img/Untitled%2020.png)

![Untitled](../img/Untitled%2021.png)

### 5.1 ì—°ê²° ì¶”ê°€~~(í•„ìš”ì—†ìŒ)~~

- ë°ì´í„°ë¥¼ ì¶”ì¶œí•  DB(Source) ê°€ Private Subnetì— ìœ„ì¹˜í•´ ìˆë‹¤ë©´ í•´ë‹¹ Private Subnet ê³¼ í†µì‹ ì´ ê°€ëŠ¥í•œ Subnetì— Glue Instanceë¥¼ ì‹¤í–‰ì‹œì¼œì•¼ë˜ëŠ”ë° ê·¸ ë•Œ ì‚¬ìš©í•˜ëŠ” ê¸°ëŠ¥
- Glue ETL ì‘ì—…ì—ì„œ `Glue â†’ Aurora(Postgresql)`ì— ì ‘ê·¼í•˜ì—¬ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì˜¤ë ¤ë©´ `JDBC` ì—°ê²°ì´ í•„ìš”.
    - Aurora(PostgreSQL) Write Instance ì˜ ì—°ê²°í•  ìˆ˜ ìˆëŠ” JDBC URL ì´ í•„ìš”.
    - ì´ë¦„
        - `{ë©”ì¼id}-handson-postgresql-connection`
    
    ![Untitled](../img/Untitled%2022.png)
    
    ![Untitled](../img/Untitled%2023.png)
    

### 5.2 Step 1. Job 1 (Dimension)

- Product ì œí’ˆ ë°ì´í„°ë¥¼ Postgresql ì—ì„œ ì¶”ì¶œí•˜ì—¬ Parquet í˜•íƒœë¡œ `Raw Data` S3ë¡œ ì €ì¥.
- Job ëª…ì¹­
    - `{ë©”ì¼id}_jb_retail_dimension_products_d2s`
    - `ê·œì¹™` : {ë©”ì¼id}_jb_í”„ë¡œì íŠ¸_íƒ€ì…_í…Œì´ë¸”ëª…_(db : d, 2:to, s : s3)

![Untitled](../img/Untitled%2024.png)

![Untitled](../img/Untitled%2025.png)

![Untitled](../img/Untitled%2026.png)

- Job Python Script
    - outputPath : S3 Path ë³€ê²½ í•„ìš”.
        - {bucket_name} â‡’ ë³¸ì¸ S3 Bucket ìœ¼ë¡œ ë³€ê²½.
    - SecretsManager ëª… ë³€ê²½ í•„ìš”.
        - {secretmanager_name} â‡’ ë³¸ì¸ secretmanager ìœ¼ë¡œ ë³€ê²½.
    
    ```python
    import sys
    from awsglue.utils import getResolvedOptions
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    from awsglue.dynamicframe import DynamicFrame
    from awsglue.job import Job
    import boto3
    from pyspark.sql import functions as F
    import json
    
    # --
    # -- SparkContext ìƒì„±
    # --
    glueContext = GlueContext(SparkContext.getOrCreate())
    spark = glueContext.spark_session
    spark.sparkContext.setLogLevel("ERROR")
    
    # --
    # -- Overwrite setting
    # --
    spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
    hadoop_conf = glueContext._jsc.hadoopConfiguration()
    hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
    #hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
    
    # --
    # -- @params: [JOB_NAME]
    # --
    args = getResolvedOptions(sys.argv, ['JOB_NAME'])
    
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    
    bucket_name = 'blee-hist-retail'
    outputPath = f's3a://{bucket_name}/dimension/products'
    
    #secretClient = boto3.client("secretsmanager", region_name="ap-northeast-2")
    secretClient = boto3.client("secretsmanager")
    
    secretmanager_name = "blee-handson-postgresql-sm2"
    get_secret_value_response = secretClient.get_secret_value(SecretId=f"{secretmanager_name}")
    secret = get_secret_value_response['SecretString']
    secret = json.loads(secret)
    
    db_username = secret.get('db_user')
    db_password = secret.get('db_pw')
    db_url = secret.get('db_url')
    
    pushdownquery = "SELECT * FROM \"RETAIL\".\"products\""
    df = spark.read.format("jdbc") \
        .option("url",db_url) \
        .option("dbtable",  "\"RETAIL\".\"products\"") \
        .option("user",db_username) \
        .option("password",db_password) \
        .option("driver","org.postgresql.Driver").load()
    
    if df.count() > 0:
        # S3ë¡œ ì ì¬
        df.write.mode('overwrite').parquet(outputPath)
    ```
    
- ê²°ê³¼
    
    ![Untitled](../img/Untitled%2027.png)
    
    ![Untitled](../img/Untitled%2028.png)
    

### 5.3 Step 1. Job 2 ( Fact Data, êµ¬ë§¤, ë°œì£¼ ë°ì´í„°)

- íŒë§¤, ë°œì£¼(íê¸°, í™˜ë¶ˆ) ë°ì´í„°ë¥¼ Postgresql ì—ì„œ ì¶”ì¶œí•˜ì—¬ Parquet í˜•íƒœë¡œ `RawData` S3ë¡œ ì €ì¥.
- ì†ì„±
    - Job ëª…ì¹­
        - {ë©”ì¼id}_jb_hist_retail_factdata_d2s
    - `ìµœëŒ€ ë™ì‹œì„±` : 2
        - StepFunctionì—ì„œ 2ê°œ jobì„ ë™ì‹œì— ëŒë¦¬ê¸° ìœ„í•´ì„œ.
    - `íŒŒë¼ë¯¸í„°`
        - íŒŒë¼ë¯¸í„° ì ìš©ì‹œ  â€œ--íŒŒë¼ë¯¸í„°ëª…â€ ìœ¼ë¡œ -- ê°€ í•„ìˆ˜ë¡œ ë“¤ì–´ê°€ì•¼ ì ìš© ë¨.
        - EXTRACT_DATE
            - ë‚ ì§œ (yyyy-MM-dd)
                - ì˜ˆ) 2015-01-02
        - TARGET_DATA
            - ê³ ì • String ê°’ 2ê°œ
                - balju (ë°œì£¼ë°ì´í„°)
                - sales (ê±°ë˜ë°ì´í„°)
    - ë‚˜ë¨¸ì§€ëŠ” ì²«ë²ˆì§¸ Job ìƒì„±ê³¼ ë™ì¼
- `í…ŒìŠ¤íŠ¸`
    - `Job 2íšŒ ì‹¤í–‰`
    - ê° ì‹¤í–‰ ë§ˆë‹¤ Job í¸ì§‘ì„ í†µí•´ `TARGET_DATA` ê°’ì„ ë³€ê²½. EXTRACT_DATE ê°’ì€ ê³ ì •.
        - balju
        - sales
- ìƒì„± ë°©ë²•ì€ ìœ„ì™€ ë™ì¼, ë‹¨ íŒŒë¼ë¯¸í„°ì—ë§Œ ì•„ë˜ì™€ ê°™ì´ ì ìš©.
    - ë³´ì•ˆ êµ¬ì„±, ìŠ¤í¬ë¦½íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì‘ì—… íŒŒë¼ë¯¸í„°(ì„ íƒí•­ëª©) ë©”ë‰´ì—ì„œ
        
        ![Untitled](../img/Untitled%2029.png)
        
- ì—°ê²°
    
    ![Untitled](../img/Untitled%2026.png)
    
- ìŠ¤í¬ë¦½íŠ¸
    - outputPath : S3 Path ë³€ê²½ í•„ìš”.
        - {bucket_name} â‡’ ë³¸ì¸ S3 Bucket ìœ¼ë¡œ ë³€ê²½.
    - SecretsManager ëª… ë³€ê²½ í•„ìš”.
        - {secretmanager_name} â‡’ ë³¸ì¸ secretmanager ìœ¼ë¡œ ë³€ê²½.
- ê²°ê³¼
    
    ```python
    import sys
    from awsglue.utils import getResolvedOptions
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    from awsglue.dynamicframe import DynamicFrame
    from awsglue.job import Job
    import boto3
    from pyspark.sql import functions as F
    import json
    
    # --
    # -- SparkContext ìƒì„±
    # --
    glueContext = GlueContext(SparkContext.getOrCreate())
    spark = glueContext.spark_session
    spark.sparkContext.setLogLevel("ERROR")
    
    # --
    # -- Overwrite setting
    # --
    spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
    hadoop_conf = glueContext._jsc.hadoopConfiguration()
    hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
    hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
    
    # --
    # -- @params: [JOB_NAME]
    # --
    args = getResolvedOptions(sys.argv, ['JOB_NAME','EXTRACT_DATE','TARGET_DATA'])
    
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    
    extract_date = args['EXTRACT_DATE']
    target_data = args['TARGET_DATA']
    
    bucket_name = 'blee-hist-retail'
    outputPath = f's3a://{bucket_name}/factdata/{target_data}'
    
    secretmanager_name = "blee-handson-postgresql-sm2" # ì´ë¦„ ë³€ê²½ í•„ìš”.
    secretClient = boto3.client("secretsmanager", region_name="ap-northeast-2")
    get_secret_value_response = secretClient.get_secret_value(SecretId=f"{secretmanager_name}")
    secret = get_secret_value_response['SecretString']
    secret = json.loads(secret)
    
    db_username = secret.get('db_user')
    db_password = secret.get('db_pw')
    db_url = secret.get('db_url')
    
    if target_data == "balju":
        db_table = "balju_refund"
        where_field = "balju_date"
    else:
        db_table = "transaction_order"
        where_field = "tr_date"
        extract_date = extract_date.replace('-','')
        
    pushdownquery = "SELECT * FROM \"RETAIL\".\"{db_table}\" WHERE {where_field} = '{extract_date}'".format(extract_date=extract_date, db_table=db_table, where_field=where_field)
    
    df = spark.read.format("jdbc") \
        .option("url",db_url) \
        .option("query",pushdownquery) \
        .option("user",db_username) \
        .option("password",db_password) \
        .option("driver","org.postgresql.Driver").load()
    
    if df.count() > 0:
        # S3ë¡œ ì ì¬ 
        df.write.mode('append').partitionBy(where_field).parquet(outputPath)
    ```
    

![Untitled](../img/Untitled%2030.png)

![Untitled](../img/Untitled%2031.png)

### 5.4 Step 1. Crawler (Step 1, Dimension/Factdata) ìƒì„±

- **`5.4.1 Dimention Product Data Crawler ìƒì„±`**
    - Crawler ëª…ì¹­
        - `ê·œì¹™` : {ë©”ì¼id}_cr_í”„ë¡œì íŠ¸_íƒ€ì…_í…Œì´ë¸”ëª…
        - blee_cr_hist_retail_dimension_products
    - `**í•„ìˆ˜**`
        - ì œì™¸ â†’ `**íŒ¨í„´ ì¶”ê°€**` (_temporary í´ë”ê°€ ìƒê¸°ë©´ì„œ Tableì´ ê¼¬ì¸ë‹¤)
        - ****/_temporary/****
    - PATH
        - s3://hist-retail/dimension/products
        - í•´ë‹¹ í´ë”ê¹Œì§€ ì§€ì •.
    
    ![Untitled](../img/Untitled%2032.png)
    
    ![Untitled](../img/Untitled%2033.png)
    
    ![Untitled](../img/Untitled%2034.png)
    
    - ì œì™¸ `**íŒ¨í„´ ì¶”ê°€**` (_temporary í´ë”ê°€ ìƒê¸°ë©´ì„œ Tableì´ ê¼¬ì¸ë‹¤)
        - ****/_temporary/****
    
    ![Untitled](../img/Untitled%2035.png)
    
    - í¬ë¡¤ëŸ¬ ì‹¤í–‰
    
    ![Untitled](../img/Untitled%2036.png)
    

- `**5.4.2 Fact Data Crawler ìƒì„± (ë°œì£¼,ê±°ë˜)**`
    - ë°œì£¼ì™€ ê±°ë˜ ë°ì´í„°ëŠ” ê°™ì€ S3 í´ë”ì— ìœ„ì¹˜í•˜ë¯€ë¡œ í•œêº¼ë²ˆì— ìˆ˜í–‰ ê°€ëŠ¥.
    - ëª…ì¹­ : {ë©”ì¼id}_cr_hist_retail_factdata_sales, balju
        - ì´ 2ê°œ í¬ë¡¤ëŸ¬ ìƒì„±.
    - `**í•„ìˆ˜**`
        - ì œì™¸ â†’ `**íŒ¨í„´ ì¶”ê°€**` (_temporary í´ë”ê°€ ìƒê¸°ë©´ì„œ Tableì´ ê¼¬ì¸ë‹¤)
        - ****/_temporary/****
    
    ![Untitled](../img/Untitled%2037.png)
    
    - í¬í•¨ ê²½ë¡œ
        - s3://blee-hist-retail/factdata/sales
        - s3://blee-hist-retail/factdata/balju
        - ì‚¬ì‹¤ ì•„ë˜ ì²˜ëŸ¼ ì§„í–‰í•´ë„ ë¬´ê´€í•˜ë‚˜ Stepfunction êµ¬ì„± ë¶€ë¶„ì—ì„œ 2ê°œë¡œ ë‚˜ëˆ ì„œ ê°œë°œí•˜ëŠ” ê²ƒìœ¼ë¡œ êµ¬ì„±ë˜ì–´ 2ê°œë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.
    
    ![Untitled](../img/Untitled%2038.png)
    
- ê²°ê³¼
    
    ![Untitled](../img/Untitled%2039.png)
    
- Athena ë°ì´í„° ì¡°íšŒ
    - í•„ìš”ì‹œ Athena ë¥¼ í†µí•´ ad-hoc ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ Data Market êµ¬ì„±ì„ ì–´ë–»ê²Œ í• ì§€ ê²°ì •.
    
    ![Untitled](../img/Untitled%2040.png)
    

### 5.5 Step 2. Silver Data ìƒì„±.

- ë°ì´í„° ìƒì„± ê¸°ì¤€(Silver)
    - ê±°ë˜ ë°ì´í„°
        - ì¼ìë³„ ë§¤ì¶œ / ê±°ë˜ ìˆ˜
        - ì¼ìë³„ ì‹œê°„ë³„ ë§¤ì¶œ / ê±°ë˜ ìˆ˜
        - ì¼ìë³„ ì‹œê°„ë³„ ì†Œë¶„ë¥˜ë³„ ë§¤ì¶œ / ê±°ë˜ ìˆ˜
        - ì¼ìë³„ ì‹œê°„ë³„ ì œí’ˆ(ìƒì„¸)ë³„ ë§¤ì¶œ / ê±°ë˜ ìˆ˜
    - ë°œì£¼ ë°ì´í„°
        - ì¼ìë³„ ì†Œë¶„ë¥˜ë³„ ë°œì£¼ / íê¸° / í™˜ë¶ˆ ìˆ˜
- ê° í•­ëª©ì— ë§ë„ë¡ Job, Crawler ìƒì„±
    - Job : ë°œì£¼, ë§¤ì¶œ 2ê°œ ìƒì„±
    - Crawler 4ê°œ ìƒì„±
        - ë°œì£¼ 1ê°œ(ì¼ë³„ ë°œì£¼)
            - blee_cr_hist_retail_silver_balju
        - ë§¤ì¶œ 1ê°œ(ì¼ì‹œê°„ë³„ ë§¤ì¶œ, ì¼ì‹œê°„ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ, ì¼ì‹œê°„ìƒì„¸ì œí’ˆë³„ ë§¤ì¶œ)
            - blee_cr_hist_retail_silver_sales
            - 1ê°œì˜ í¬ë¡¤ëŸ¬ë¡œ 3ê°œ Table í¬ë¡¤ë§.

### 5.6 Step 2. Job ìƒì„± ( ë°œì£¼ ë°ì´í„° )

- ëª…ì¹­
    - blee_jb_hist_retail_silver_dailybalju_s2s
- ì†ì„±
    - `íŒŒë¼ë¯¸í„°`
        - EXTRACT_DATE
            - ë‚ ì§œ (2015-01-01)
    - ì—°ê²°
        - ì„ íƒ í•„ìš” ì—†ìŒ
        - PostgreSQL ì ‘ê·¼ì´ ì—†ê¸° ë•Œë¬¸ì— ìœ„ì— ìƒì„±í–ˆë˜ Jobê³¼ ë‹¤ë¥´ê²Œ ì—°ê²°ì„ ì„ íƒí•  í•„ìš” ì—†ìŒ.
- í™”ë©´ ìº¡ì²˜
    
    ![Untitled](../img/Untitled%2041.png)
    
    ![Untitled](../img/Untitled%2042.png)
    
- **Script**
    - `bucket_name ë³€ê²½ í•„ìš”.` â‡’ ë³¸ì¸ S3 Bucket ìœ¼ë¡œ ë³€ê²½.

```python
import sys
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
import boto3
from pyspark.sql import functions as func
import json

from botocore.exceptions import ClientError
from pyspark.sql.functions import sum

args = getResolvedOptions(sys.argv, ['JOB_NAME','EXTRACT_DATE'])

bucket_name = 'blee-hist-retail' # ì´ë¦„ ë³€ê²½ í•„ìš”.
outputPath = f"s3a://{bucket_name}/silver/dailybalju"

# --
# -- SparkContext ìƒì„±
# --
glueContext = GlueContext(SparkContext.getOrCreate())
spark = glueContext.spark_session
spark.sparkContext.setLogLevel("ERROR")

spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
hadoop_conf = glueContext._jsc.hadoopConfiguration()
hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 

job = Job(glueContext)
job.init(args['JOB_NAME'], args)

product_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = 'hist-retail', table_name = 'products')
product_df = product_dynamic_frame.toDF()

product_category_df = product_df.drop("product_cd").drop("product_name").drop_duplicates()

etl_dt = args['EXTRACT_DATE']

balju_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = 'hist-retail', table_name = 'balju', push_down_predicate = "(balju_date in ('{}'))".format(etl_dt))
balju_df = balju_dynamic_frame.toDF()

balju_df2 = balju_df.withColumn("balju_qty", balju_df.balju_qty.cast('int')).withColumn("refund_qty", balju_df.refund_qty.cast('int')).withColumn('disposal_qty', balju_df.disposal_qty.cast('int'))
integ_df = balju_df2.join(product_df, balju_df.product_cd == product_df.product_cd, "inner").drop(product_df.product_cd)

daily_summary_of_balju_df = integ_df.groupBy(["balju_date","subcategory_cd"]).agg(sum("balju_qty").alias("sum_balju"),sum("refund_qty").alias("sum_refund"),sum("disposal_qty").alias("sum_disposal"))

daily_summary_of_balju_df2 = daily_summary_of_balju_df.sort(daily_summary_of_balju_df.balju_date, daily_summary_of_balju_df.subcategory_cd). \
                                join(product_category_df,daily_summary_of_balju_df.subcategory_cd == product_category_df.subcategory_cd, how="INNER"). \
                                drop(daily_summary_of_balju_df.subcategory_cd)

if daily_summary_of_balju_df2.count() > 0:
    daily_summary_of_balju_df2.write.mode('append').partitionBy("balju_date").parquet(outputPath)
```

![Untitled](../img/Untitled%2043.png)

### 5.7 Step 2. Job ìƒì„± ( ë§¤ì¶œ ë°ì´í„° )

- ëª…ì¹­
    - {ë©”ì¼id}_jb_hist_retail_silver_dailysales_s2s
- ì†ì„±
    - `íŒŒë¼ë¯¸í„°`
        - EXTRACT_DATE
            - ë‚ ì§œ (2015-01-01)
    - ì—°ê²°
        - ì„ íƒ í•„ìš” ì—†ìŒ
        - PostgreSQL ì ‘ê·¼ì´ ì—†ê¸° ë•Œë¬¸ì— ìœ„ì— ìƒì„±í–ˆë˜ Jobê³¼ ë‹¤ë¥´ê²Œ ì—°ê²°ì„ ì„ íƒí•  í•„ìš” ì—†ìŒ
- í™”ë©´ ìº¡ì²˜
    
    ![Untitled](../img/Untitled%2044.png)
    
    ![Untitled](../img/Untitled%2045.png)
    
- Script

```python
import sys
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
import boto3
from pyspark.sql import functions as func
import json

from botocore.exceptions import ClientError
from pyspark.sql.functions import sum, avg, count

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'EXTRACT_DATE'])

bucket_name = 'blee-hist-retail'
outputPath = f"s3a://{bucket_name}/silver/sales"

# --
# -- SparkContext ìƒì„±
# --
glueContext = GlueContext(SparkContext.getOrCreate())
spark = glueContext.spark_session
spark.sparkContext.setLogLevel("ERROR")

spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
hadoop_conf = glueContext._jsc.hadoopConfiguration()
hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 

job = Job(glueContext)
job.init(args['JOB_NAME'], args)

product_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = 'hist-retail', table_name = 'products')
product_df = product_dynamic_frame.toDF()

product_category_df = product_df.drop("product_cd").drop("product_name").drop("is_pb").drop_duplicates()
#product_category_df.show(3)

etl_dt = args['EXTRACT_DATE']
order_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = 'hist-retail', table_name = 'sales', push_down_predicate = "(tr_date in ('{}'))".format(etl_dt))
order_df = order_dynamic_frame.toDF()

order_df2 = order_df.withColumn('tr_time',order_df.tr_time.cast('int'))
order_df3 = order_df2.join(product_df, order_df2.product_cd == product_df.product_cd, how="inner").drop(order_df2.product_cd)

# Daily + Time ë§¤ì¶œ
datetime_sales_df = order_df3.groupby(["tr_date","tr_time"]).agg(sum("mount").alias("sum_sales"),count("mount").alias("count_sales")).sort(func.col("tr_time"))
datetime_sales_df.write.mode('append').partitionBy("tr_date").parquet(outputPath+"/dailytime")

# Daily + Time + ì†Œë¶„ë¥˜ë³„ ë§¤ì¶œ
datetime_cate_sales_df = order_df3.groupby(["tr_date","tr_time","subcategory_cd"]).agg(sum("mount").alias("sum_sales"),count("mount").alias("count_sales")).sort(func.col("tr_time"),func.col("sum_sales").desc())
datetime_cate_sales_df2 = datetime_cate_sales_df.join(product_category_df, datetime_cate_sales_df.subcategory_cd == product_category_df.subcategory_cd, how="left").drop(datetime_cate_sales_df.subcategory_cd)
datetime_cate_sales_df2.write.mode('append').partitionBy("tr_date").parquet(outputPath+"/dailytimeSubcate")

# Daily + Timely + ìƒì„¸ ì œí’ˆë³„ ë§¤ì¶œ
datetime_product_sales_df = order_df3.groupby(["tr_date","tr_time","product_cd"]).agg(sum("mount").alias("sum_sales"),count("mount").alias("count_sales")).sort(func.col("tr_time"),func.col("sum_sales").desc())
datetime_product_sales_df2 = datetime_product_sales_df.join(product_df, datetime_product_sales_df.product_cd == product_df.product_cd, how="left").drop(datetime_product_sales_df.product_cd)
datetime_product_sales_df2.write.mode('append').partitionBy("tr_date").parquet(outputPath+"/dailytimeProduct")
```

![Untitled](../img/Untitled%2046.png)

### 5.8 Step 2. Crawler ìƒì„±

- 4ê°€ì§€ ìƒì„±
    - `ë°œì£¼`
        - ì´ë¦„ : blee_cr_hist_retail_silver_balju
        - í¬í•¨ ê²½ë¡œ : s3://blee-hist-retail/silver/dailybalju/
    - `íŒë§¤(3ê°œ)`
        - ì¼ë³„ì‹œê°„ë³„ ë§¤ì¶œ
            - ì´ë¦„ : blee_cr_hist_retail_silver_sales_dailytime
            - í¬í•¨ ê²½ë¡œ : s3://blee-hist-retail/silver/sales/dailytime/
        - ì¼ë³„ì‹œê°„ë³„ì†Œë¶„ë¥˜ë³„ ë§¤ì¶œ
            - ì´ë¦„ : blee_cr_hist_retail_silver_sales_dailysubcate
            - í¬í•¨ ê²½ë¡œ : s3://blee-hist-retail/silver/sales/dailytimeSubcate/
        - ì¼ë³„ì‹œê°„ë³„ì œí’ˆë³„ ë§¤ì¶œ
            - ì´ë¦„ : blee_cr_hist_retail_silver_sales_dailyproduct
            - í¬í•¨ ê²½ë¡œ : s3://blee-hist-retail/silver/sales/dailytimeProduct/
- ë°œì£¼ Crawler ìƒì„±
    - `**í•„ìˆ˜**`
        - ì œì™¸ â†’ `**íŒ¨í„´ ì¶”ê°€**` (_temporary í´ë”ê°€ ìƒê¸°ë©´ì„œ Tableì´ ê¼¬ì¸ë‹¤)
        - ****/_temporary/****
    - IAM ì—­í• 
        - blee-handson-glue-role
        - ì´ì „ì— ìƒì„±í–ˆë˜ Role
    - í™”ë©´ ìº¡ì²˜
        
        ![Untitled](../img/Untitled%2047.png)
        
        ![Untitled](../img/Untitled%2048.png)
        
        ![Untitled](../img/Untitled%2049.png)
        
        ![Untitled](../img/Untitled%2050.png)
        
- íŒë§¤ Crawler ìƒì„±
    - (Legacy Page) ê°ê° ìœ„ì— ì‘ì„±ëœ ê²½ë¡œ 3ê°œ ë°œì£¼ì™€ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ìƒì„±
    - (ì‹ ê·œí˜ì´ì§€) í•˜ë‚˜ì˜ í¬ë¡¤ëŸ¬ë¡œ 3ê°œ í…Œì´ë¸” ë™ì‹œ í¬ë¡¤ë§ ê°€ëŠ¥
        - `(ì˜µì…˜)íŒë§¤ Crawler ìƒì„±`
            - `ì‹ ê·œ Crawler ë©”ë‰´ ì„ íƒ`
            - Create Crawler
            
            ![Untitled](../img/Untitled%2051.png)
            
            - Add a data source â†’ Create Crawler
                - ì¼ë³„ì‹œê°„ë³„ ë§¤ì¶œ
                    - í¬í•¨ ê²½ë¡œ : s3://blee-hist-retail/silver/sales/dailytime/
                - ì¼ë³„ì‹œê°„ë³„ì†Œë¶„ë¥˜ë³„ ë§¤ì¶œ
                    - í¬í•¨ ê²½ë¡œ : s3://blee-hist-retail/silver/sales/dailytimeSubcate/
                - ì¼ë³„ì‹œê°„ë³„ì œí’ˆë³„ ë§¤ì¶œ
                    - í¬í•¨ ê²½ë¡œ : s3://blee-hist-retail/silver/sales/dailytimeProduct/
                - ì•„ë˜ì™€ ê°™ì´ 3ê°œ í•­ëª©ì— ëŒ€í•´ì„œ Add a Datasource
                
                ![Untitled](../img/Untitled%2052.png)
                
                ![Untitled](../img/Untitled%2053.png)
                
                ![Untitled](../img/Untitled%2054.png)
                
            - ì œì™¸ â†’ `**íŒ¨í„´ ì¶”ê°€**` (_temporary í´ë”ê°€ ìƒê¸°ë©´ì„œ Tableì´ ê¼¬ì¸ë‹¤)
            - ****/_temporary/****
- ê²°ê³¼

![Untitled](../img/Untitled%2055.png)